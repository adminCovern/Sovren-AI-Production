#include "inference_engine.h"
#include <iostream>
#include <vector>
#include <string>
#include <chrono>

void print_banner() {
    std::cout << R"(
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SOVREN B200 ENGINE                        â•‘
â•‘              Next-Generation AI Inference Engine             â•‘
â•‘                                                               â•‘
â•‘  ðŸš€ Optimized for NVIDIA B200 Architecture                   â•‘
â•‘  âš¡ Multi-GPU Tensor Parallelism                             â•‘
â•‘  ðŸ§  Advanced Memory Management                               â•‘
â•‘  ðŸ”¥ Ultra-Low Latency Inference                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
)" << std::endl;
}

void print_system_info() {
    std::cout << "\n=== System Information ===" << std::endl;
    
#if CUDA_AVAILABLE
    std::cout << "âœ… CUDA Support: ENABLED" << std::endl;
    std::cout << "ðŸŽ¯ Target Architecture: B200 (sm_100)" << std::endl;
    std::cout << "ðŸ”§ Optimization Level: Maximum Performance" << std::endl;
#else
    std::cout << "âš ï¸  CUDA Support: DISABLED (CPU-only mode)" << std::endl;
    std::cout << "ðŸ’» Running in CPU fallback mode" << std::endl;
#endif
    
    std::cout << "ðŸ—ï¸  Build Configuration: " << 
#ifdef NDEBUG
        "Release"
#else
        "Debug"
#endif
        << std::endl;
}

int main(int argc, char* argv[]) {
    print_banner();
    print_system_info();
    
    try {
        // Initialize the inference engine
        std::cout << "\nðŸš€ Initializing SOVREN B200 Inference Engine..." << std::endl;
        
        int num_gpus = 8; // Default for B200 setup
        if (argc > 1) {
            num_gpus = std::stoi(argv[1]);
        }
        
        SOVRENInferenceEngine engine(num_gpus);
        
        // Initialize the engine
        auto start_time = std::chrono::high_resolution_clock::now();
        
        if (!engine.initialize()) {
            std::cerr << "âŒ Failed to initialize inference engine!" << std::endl;
            return -1;
        }
        
        auto init_time = std::chrono::high_resolution_clock::now();
        auto init_duration = std::chrono::duration_cast<std::chrono::milliseconds>(
            init_time - start_time).count();
        
        std::cout << "âœ… Engine initialized successfully in " << init_duration << "ms" << std::endl;
        
        // Load model (placeholder path)
        std::string model_path = "models/sovren_b200_model";
        if (argc > 2) {
            model_path = argv[2];
        }
        
        std::cout << "\nðŸ“¦ Loading model from: " << model_path << std::endl;
        if (!engine.load_model(model_path)) {
            std::cerr << "âŒ Failed to load model!" << std::endl;
            return -1;
        }
        
        // Setup multi-GPU if available
        if (num_gpus > 1) {
            std::cout << "\nðŸ”— Setting up multi-GPU configuration..." << std::endl;
            if (!engine.setup_multi_gpu()) {
                std::cerr << "âš ï¸  Multi-GPU setup failed, continuing with single GPU" << std::endl;
            }
        }
        
        // Warmup kernels
        std::cout << "\nðŸ”¥ Warming up inference kernels..." << std::endl;
        engine.warmup_kernels();
        
        // Optimize for performance
        std::cout << "\nâš¡ Optimizing for maximum throughput..." << std::endl;
        engine.optimize_for_throughput();
        
        // Test inference with sample input
        std::cout << "\nðŸ§ª Running test inference..." << std::endl;
        std::vector<int> test_tokens = {1, 2, 3, 4, 5}; // Sample tokens
        std::vector<int> output_tokens;
        
        auto inference_start = std::chrono::high_resolution_clock::now();
        
        bool success = engine.generate_tokens(
            test_tokens, 
            output_tokens, 
            10,    // max_new_tokens
            0.7f,  // temperature
            0.9f   // top_p
        );
        
        auto inference_end = std::chrono::high_resolution_clock::now();
        auto inference_duration = std::chrono::duration_cast<std::chrono::microseconds>(
            inference_end - inference_start).count();
        
        if (success) {
            std::cout << "âœ… Test inference completed successfully!" << std::endl;
            std::cout << "â±ï¸  Inference time: " << inference_duration << " microseconds" << std::endl;
            std::cout << "ðŸŽ¯ Tokens per second: " << 
                (output_tokens.size() * 1000000.0 / inference_duration) << std::endl;
        } else {
            std::cerr << "âŒ Test inference failed!" << std::endl;
        }
        
        // Print performance statistics
        std::cout << "\n" << std::endl;
        engine.print_performance_stats();
        
        // Validate model integrity
        std::cout << "\nðŸ” Validating model integrity..." << std::endl;
        if (engine.validate_model_integrity()) {
            std::cout << "âœ… Model validation passed!" << std::endl;
        } else {
            std::cerr << "âš ï¸  Model validation warnings detected" << std::endl;
        }
        
        std::cout << "\nðŸŽ‰ SOVREN B200 Engine ready for production!" << std::endl;
        std::cout << "ðŸ’¡ Use this engine for ultra-fast AI inference at scale" << std::endl;
        
        // Keep running for server mode (placeholder)
        std::cout << "\nâ¸ï¸  Press Ctrl+C to shutdown..." << std::endl;
        
        // In a real implementation, this would start the HTTP server
        // and handle inference requests
        
        return 0;
        
    } catch (const std::exception& e) {
        std::cerr << "ðŸ’¥ Fatal error: " << e.what() << std::endl;
        return -1;
    } catch (...) {
        std::cerr << "ðŸ’¥ Unknown fatal error occurred!" << std::endl;
        return -1;
    }
}
