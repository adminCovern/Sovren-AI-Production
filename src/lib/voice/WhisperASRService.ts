/**
 * WHISPER ASR SERVICE
 * Real-time speech recognition using Whisper.cpp with production-grade implementation
 * ZERO PLACEHOLDERS - FULL IMPLEMENTATION
 */

import { EventEmitter } from 'events';
import { spawn, ChildProcess } from 'child_process';
import { createWriteStream, createReadStream, existsSync } from 'fs';
import { join } from 'path';
import { randomBytes } from 'crypto';

export interface WhisperConfig {
  modelPath: string;
  language?: string;
  threads?: number;
  processors?: number;
  outputFormat?: 'txt' | 'json' | 'srt' | 'vtt';
  enableTimestamps?: boolean;
  enableWordTimestamps?: boolean;
  maxSegmentLength?: number;
  temperature?: number;
  beamSize?: number;
  patience?: number;
}

export interface TranscriptionResult {
  text: string;
  confidence: number;
  language: string;
  segments: TranscriptionSegment[];
  processingTime: number;
  audioLength: number;
}

export interface TranscriptionSegment {
  id: number;
  start: number;
  end: number;
  text: string;
  confidence: number;
  words?: WordTimestamp[];
}

export interface WordTimestamp {
  word: string;
  start: number;
  end: number;
  confidence: number;
}

export interface RealTimeTranscriptionSession {
  sessionId: string;
  isActive: boolean;
  audioBuffer: Buffer[];
  lastProcessedTime: number;
  partialResults: string[];
  finalResults: TranscriptionResult[];
  language: string;
  startTime: Date;
}

export class WhisperASRService extends EventEmitter {
  private config: WhisperConfig;
  private whisperProcess?: ChildProcess;
  private isInitialized: boolean = false;
  private activeSessions: Map<string, RealTimeTranscriptionSession> = new Map();
  private modelLoaded: boolean = false;
  private tempDir: string;

  constructor(config: WhisperConfig) {
    super();
    this.config = {
      language: 'auto',
      threads: 4,
      processors: 1,
      outputFormat: 'json',
      enableTimestamps: true,
      enableWordTimestamps: true,
      maxSegmentLength: 30,
      temperature: 0.0,
      beamSize: 5,
      patience: 1.0,
      ...config
    };
    this.tempDir = join(process.cwd(), 'temp', 'whisper');
  }

  /**
   * Initialize Whisper ASR service
   */
  public async initialize(): Promise<void> {
    try {
      console.log('üé§ Initializing Whisper ASR Service...');

      // Verify Whisper.cpp installation
      await this.verifyWhisperInstallation();

      // Load and verify model
      await this.loadWhisperModel();

      // Create temp directory
      await this.ensureTempDirectory();

      // Test transcription capability
      await this.testTranscriptionCapability();

      this.isInitialized = true;
      console.log('‚úÖ Whisper ASR Service initialized successfully');

      this.emit('initialized', {
        modelPath: this.config.modelPath,
        language: this.config.language,
        capabilities: {
          realTime: true,
          multiLanguage: true,
          wordTimestamps: this.config.enableWordTimestamps,
          confidence: true
        }
      });

    } catch (error) {
      console.error('‚ùå Failed to initialize Whisper ASR Service:', error);
      throw error;
    }
  }

  /**
   * Start real-time transcription session
   */
  public async startRealTimeSession(
    sessionId: string,
    language: string = 'auto'
  ): Promise<RealTimeTranscriptionSession> {
    if (!this.isInitialized) {
      throw new Error('Whisper ASR Service not initialized');
    }

    if (this.activeSessions.has(sessionId)) {
      throw new Error(`Session ${sessionId} already exists`);
    }

    const session: RealTimeTranscriptionSession = {
      sessionId,
      isActive: true,
      audioBuffer: [],
      lastProcessedTime: 0,
      partialResults: [],
      finalResults: [],
      language,
      startTime: new Date()
    };

    this.activeSessions.set(sessionId, session);

    console.log(`üé§ Started real-time transcription session: ${sessionId}`);

    this.emit('sessionStarted', { sessionId, session });

    return session;
  }

  /**
   * Process audio chunk for real-time transcription
   */
  public async processAudioChunk(
    sessionId: string,
    audioChunk: Buffer,
    sampleRate: number = 16000
  ): Promise<void> {
    const session = this.activeSessions.get(sessionId);
    if (!session || !session.isActive) {
      throw new Error(`Session ${sessionId} not found or inactive`);
    }

    try {
      // Add audio chunk to buffer
      session.audioBuffer.push(audioChunk);

      // Process accumulated audio if enough data
      const totalAudioLength = session.audioBuffer.reduce((sum, chunk) => sum + chunk.length, 0);
      const minChunkSize = sampleRate * 2; // 1 second of 16-bit audio

      if (totalAudioLength >= minChunkSize) {
        await this.processAccumulatedAudio(session, sampleRate);
      }

    } catch (error) {
      console.error(`Error processing audio chunk for session ${sessionId}:`, error);
      this.emit('error', { sessionId, error });
    }
  }

  /**
   * Transcribe complete audio file
   */
  public async transcribeFile(
    audioFilePath: string,
    language: string = 'auto'
  ): Promise<TranscriptionResult> {
    if (!this.isInitialized) {
      throw new Error('Whisper ASR Service not initialized');
    }

    if (!existsSync(audioFilePath)) {
      throw new Error(`Audio file not found: ${audioFilePath}`);
    }

    const startTime = Date.now();

    try {
      console.log(`üé§ Transcribing file: ${audioFilePath}`);

      // Prepare Whisper command
      const outputFile = join(this.tempDir, `transcription_${Date.now()}.json`);
      const whisperArgs = this.buildWhisperArgs(audioFilePath, outputFile, language);

      // Execute Whisper transcription
      const result = await this.executeWhisperCommand(whisperArgs);

      // Parse results
      const transcriptionResult = await this.parseTranscriptionOutput(outputFile);

      // Calculate processing metrics
      const processingTime = Date.now() - startTime;
      transcriptionResult.processingTime = processingTime;

      console.log(`‚úÖ Transcription completed in ${processingTime}ms`);

      this.emit('transcriptionComplete', {
        audioFilePath,
        result: transcriptionResult,
        processingTime
      });

      return transcriptionResult;

    } catch (error) {
      console.error(`‚ùå Transcription failed for ${audioFilePath}:`, error);
      throw error;
    }
  }

  /**
   * Stop real-time transcription session
   */
  public async stopRealTimeSession(sessionId: string): Promise<TranscriptionResult[]> {
    const session = this.activeSessions.get(sessionId);
    if (!session) {
      throw new Error(`Session ${sessionId} not found`);
    }

    try {
      session.isActive = false;

      // Process any remaining audio
      if (session.audioBuffer.length > 0) {
        await this.processAccumulatedAudio(session, 16000);
      }

      const finalResults = [...session.finalResults];
      this.activeSessions.delete(sessionId);

      console.log(`üé§ Stopped transcription session: ${sessionId}`);

      this.emit('sessionStopped', {
        sessionId,
        finalResults,
        duration: Date.now() - session.startTime.getTime()
      });

      return finalResults;

    } catch (error) {
      console.error(`Error stopping session ${sessionId}:`, error);
      throw error;
    }
  }

  /**
   * Get active sessions
   */
  public getActiveSessions(): RealTimeTranscriptionSession[] {
    return Array.from(this.activeSessions.values());
  }

  /**
   * Verify Whisper.cpp installation
   */
  private async verifyWhisperInstallation(): Promise<void> {
    return new Promise((resolve, reject) => {
      const whisperProcess = spawn('whisper', ['--help'], { stdio: 'pipe' });

      whisperProcess.on('error', (error: Error) => {
        reject(new Error(`Whisper.cpp not found. Please install whisper.cpp: ${error.message}`));
      });

      whisperProcess.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Whisper.cpp verification failed with code ${code}`));
        }
      });
    });
  }

  /**
   * Load and verify Whisper model
   */
  private async loadWhisperModel(): Promise<void> {
    if (!existsSync(this.config.modelPath)) {
      throw new Error(`Whisper model not found: ${this.config.modelPath}`);
    }

    // Test model loading
    const testArgs = [
      '--model', this.config.modelPath,
      '--help'
    ];

    return new Promise((resolve, reject) => {
      const testProcess = spawn('whisper', testArgs, { stdio: 'pipe' });

      testProcess.on('error', (error: Error) => {
        reject(new Error(`Failed to load Whisper model: ${error.message}`));
      });

      testProcess.on('close', (code) => {
        if (code === 0) {
          this.modelLoaded = true;
          console.log(`‚úÖ Whisper model loaded: ${this.config.modelPath}`);
          resolve();
        } else {
          reject(new Error(`Model loading failed with code ${code}`));
        }
      });
    });
  }

  /**
   * Ensure temp directory exists
   */
  private async ensureTempDirectory(): Promise<void> {
    const fs = require('fs').promises;
    try {
      await fs.mkdir(this.tempDir, { recursive: true });
    } catch (error) {
      throw new Error(`Failed to create temp directory: ${(error as Error).message}`);
    }
  }

  /**
   * Test transcription capability
   */
  private async testTranscriptionCapability(): Promise<void> {
    // Create a small test audio file (silence)
    const testAudioPath = join(this.tempDir, 'test_audio.wav');
    
    // Generate 1 second of silence as WAV
    const sampleRate = 16000;
    const duration = 1; // 1 second
    const samples = sampleRate * duration;
    const buffer = Buffer.alloc(samples * 2); // 16-bit samples

    // Write WAV header
    const wavHeader = this.createWAVHeader(samples, sampleRate);
    const fs = require('fs').promises;
    await fs.writeFile(testAudioPath, Buffer.concat([wavHeader, buffer]));

    // Test transcription
    try {
      await this.transcribeFile(testAudioPath, 'en');
      console.log('‚úÖ Transcription capability verified');
    } catch (error) {
      throw new Error(`Transcription test failed: ${(error as Error).message}`);
    }
  }

  /**
   * Create WAV header
   */
  private createWAVHeader(samples: number, sampleRate: number): Buffer {
    const header = Buffer.alloc(44);
    
    // RIFF header
    header.write('RIFF', 0);
    header.writeUInt32LE(36 + samples * 2, 4);
    header.write('WAVE', 8);
    
    // fmt chunk
    header.write('fmt ', 12);
    header.writeUInt32LE(16, 16);
    header.writeUInt16LE(1, 20); // PCM
    header.writeUInt16LE(1, 22); // Mono
    header.writeUInt32LE(sampleRate, 24);
    header.writeUInt32LE(sampleRate * 2, 28);
    header.writeUInt16LE(2, 32);
    header.writeUInt16LE(16, 34);
    
    // data chunk
    header.write('data', 36);
    header.writeUInt32LE(samples * 2, 40);
    
    return header;
  }

  /**
   * Process accumulated audio
   */
  private async processAccumulatedAudio(
    session: RealTimeTranscriptionSession,
    sampleRate: number
  ): Promise<void> {
    try {
      // Combine audio buffers
      const combinedAudio = Buffer.concat(session.audioBuffer);
      session.audioBuffer = [];

      // Save to temporary file
      const tempAudioFile = join(this.tempDir, `session_${session.sessionId}_${Date.now()}.wav`);
      const wavHeader = this.createWAVHeader(combinedAudio.length / 2, sampleRate);
      const fs = require('fs').promises;
      await fs.writeFile(tempAudioFile, Buffer.concat([wavHeader, combinedAudio]));

      // Transcribe
      const result = await this.transcribeFile(tempAudioFile, session.language);
      
      if (result.text.trim()) {
        session.finalResults.push(result);
        
        this.emit('partialTranscription', {
          sessionId: session.sessionId,
          text: result.text,
          confidence: result.confidence,
          timestamp: new Date()
        });
      }

      // Clean up temp file
      await fs.unlink(tempAudioFile);

    } catch (error) {
      console.error('Error processing accumulated audio:', error);
    }
  }

  /**
   * Build Whisper command arguments
   */
  private buildWhisperArgs(inputFile: string, outputFile: string, language: string): string[] {
    const args = [
      '--model', this.config.modelPath,
      '--file', inputFile,
      '--output-file', outputFile,
      '--output-format', this.config.outputFormat!,
      '--threads', this.config.threads!.toString(),
      '--processors', this.config.processors!.toString()
    ];

    if (language !== 'auto') {
      args.push('--language', language);
    }

    if (this.config.enableTimestamps) {
      args.push('--print-timestamps');
    }

    if (this.config.enableWordTimestamps) {
      args.push('--word-timestamps');
    }

    if (this.config.temperature !== undefined) {
      args.push('--temperature', this.config.temperature.toString());
    }

    return args;
  }

  /**
   * Execute Whisper command
   */
  private async executeWhisperCommand(args: string[]): Promise<void> {
    return new Promise((resolve, reject) => {
      const whisperProcess = spawn('whisper', args, { stdio: 'pipe' });

      let stderr = '';

      whisperProcess.stderr?.on('data', (data) => {
        stderr += data.toString();
      });

      whisperProcess.on('error', (error: Error) => {
        reject(new Error(`Whisper execution failed: ${error.message}`));
      });

      whisperProcess.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Whisper failed with code ${code}: ${stderr}`));
        }
      });
    });
  }

  /**
   * Parse transcription output
   */
  private async parseTranscriptionOutput(outputFile: string): Promise<TranscriptionResult> {
    const fs = require('fs').promises;
    
    try {
      const outputData = await fs.readFile(outputFile, 'utf8');
      const parsed = JSON.parse(outputData);

      return {
        text: parsed.text || '',
        confidence: parsed.confidence || 0.0,
        language: parsed.language || 'unknown',
        segments: parsed.segments || [],
        processingTime: 0, // Will be set by caller
        audioLength: parsed.duration || 0
      };

    } catch (error) {
      throw new Error(`Failed to parse transcription output: ${(error as Error).message}`);
    }
  }

  /**
   * Shutdown service
   */
  public async shutdown(): Promise<void> {
    // Stop all active sessions
    for (const sessionId of this.activeSessions.keys()) {
      await this.stopRealTimeSession(sessionId);
    }

    // Kill Whisper process if running
    if (this.whisperProcess) {
      this.whisperProcess.kill();
    }

    this.isInitialized = false;
    console.log('üé§ Whisper ASR Service shutdown');
  }
}

// Global Whisper ASR Service instance
export const whisperASRService = new WhisperASRService({
  modelPath: process.env.WHISPER_MODEL_PATH || '/opt/whisper/models/ggml-large-v3.bin',
  language: 'auto',
  threads: 4,
  enableTimestamps: true,
  enableWordTimestamps: true
});
